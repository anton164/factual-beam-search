{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prior prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/bart-large were not used when initializing BartForCausalLM: ['encoder.layers.0.fc2.weight', 'encoder.layers.1.self_attn.out_proj.bias', 'encoder.layers.4.self_attn.out_proj.bias', 'encoder.layers.8.self_attn.q_proj.weight', 'encoder.layers.10.self_attn_layer_norm.weight', 'encoder.layers.0.final_layer_norm.bias', 'encoder.layers.4.fc2.bias', 'encoder.layers.4.final_layer_norm.bias', 'encoder.layers.7.fc1.bias', 'encoder.layers.4.self_attn.q_proj.weight', 'encoder.layers.9.fc1.weight', 'encoder.layers.5.self_attn.out_proj.bias', 'encoder.layers.6.self_attn.out_proj.weight', 'encoder.layers.1.fc1.bias', 'encoder.layers.5.self_attn.q_proj.bias', 'encoder.layers.4.self_attn.v_proj.weight', 'encoder.layers.10.fc2.weight', 'encoder.layers.10.self_attn_layer_norm.bias', 'encoder.layers.6.self_attn.k_proj.bias', 'encoder.layers.3.self_attn.out_proj.bias', 'encoder.layers.4.self_attn.q_proj.bias', 'encoder.layers.5.self_attn.q_proj.weight', 'encoder.layers.6.fc1.bias', 'encoder.layers.7.fc2.weight', 'encoder.layers.4.final_layer_norm.weight', 'encoder.layers.8.self_attn_layer_norm.weight', 'encoder.layers.6.fc1.weight', 'encoder.layers.9.final_layer_norm.weight', 'encoder.layers.11.self_attn.k_proj.weight', 'encoder.layers.7.self_attn.k_proj.bias', 'encoder.layers.9.self_attn_layer_norm.weight', 'encoder.layers.8.self_attn_layer_norm.bias', 'encoder.layers.2.self_attn.k_proj.weight', 'encoder.layers.9.self_attn_layer_norm.bias', 'encoder.layers.5.fc2.weight', 'encoder.layers.10.self_attn.v_proj.bias', 'encoder.layers.0.self_attn_layer_norm.weight', 'encoder.layers.1.final_layer_norm.weight', 'encoder.layers.1.fc1.weight', 'encoder.layers.6.fc2.bias', 'encoder.layernorm_embedding.weight', 'encoder.layers.11.self_attn.q_proj.weight', 'encoder.layers.6.fc2.weight', 'encoder.layers.11.final_layer_norm.bias', 'encoder.layers.1.final_layer_norm.bias', 'encoder.layers.7.self_attn_layer_norm.weight', 'encoder.layers.1.self_attn.v_proj.bias', 'encoder.layers.6.self_attn.v_proj.bias', 'encoder.layers.0.self_attn.v_proj.bias', 'encoder.layers.0.self_attn.out_proj.bias', 'encoder.layers.11.self_attn.out_proj.bias', 'encoder.layers.10.fc1.bias', 'encoder.layers.11.self_attn_layer_norm.bias', 'encoder.layers.0.fc1.weight', 'encoder.layers.9.self_attn.out_proj.weight', 'encoder.layers.8.self_attn.k_proj.weight', 'encoder.layers.11.fc2.bias', 'encoder.layers.6.self_attn_layer_norm.weight', 'encoder.layers.3.final_layer_norm.bias', 'encoder.layers.4.self_attn_layer_norm.weight', 'encoder.layers.11.self_attn.out_proj.weight', 'encoder.layers.5.final_layer_norm.bias', 'encoder.layers.8.self_attn.v_proj.weight', 'encoder.layers.9.self_attn.v_proj.bias', 'encoder.layers.8.fc1.bias', 'encoder.layers.10.self_attn.q_proj.weight', 'encoder.layers.1.self_attn.k_proj.bias', 'encoder.layers.2.self_attn_layer_norm.bias', 'encoder.layers.8.self_attn.out_proj.bias', 'encoder.layers.8.self_attn.q_proj.bias', 'encoder.layers.7.self_attn_layer_norm.bias', 'encoder.layers.0.self_attn_layer_norm.bias', 'encoder.layers.2.self_attn.v_proj.weight', 'encoder.layers.9.self_attn.k_proj.bias', 'encoder.layers.11.self_attn.q_proj.bias', 'encoder.layers.4.fc2.weight', 'encoder.layers.2.self_attn.out_proj.weight', 'encoder.layers.9.self_attn.v_proj.weight', 'encoder.layers.5.fc1.weight', 'encoder.layers.1.self_attn.k_proj.weight', 'encoder.layers.3.fc1.bias', 'encoder.layers.0.self_attn.q_proj.bias', 'encoder.layers.11.fc1.weight', 'encoder.layers.1.self_attn.out_proj.weight', 'encoder.embed_positions.weight', 'encoder.layers.6.final_layer_norm.weight', 'encoder.layers.3.self_attn.v_proj.weight', 'encoder.layers.2.fc2.bias', 'encoder.layers.1.fc2.weight', 'encoder.layers.3.fc2.bias', 'encoder.layers.2.self_attn_layer_norm.weight', 'encoder.layers.7.self_attn.q_proj.weight', 'encoder.layers.4.self_attn.out_proj.weight', 'encoder.layers.8.fc2.bias', 'encoder.layers.8.self_attn.v_proj.bias', 'encoder.layers.4.self_attn.k_proj.bias', 'encoder.layers.7.fc2.bias', 'encoder.layers.5.fc2.bias', 'encoder.layers.0.self_attn.v_proj.weight', 'encoder.layers.7.final_layer_norm.bias', 'encoder.layers.6.self_attn.k_proj.weight', 'encoder.layers.9.self_attn.k_proj.weight', 'encoder.layers.3.self_attn_layer_norm.weight', 'encoder.layers.9.fc2.weight', 'encoder.layers.6.self_attn.q_proj.bias', 'encoder.layers.5.fc1.bias', 'encoder.embed_tokens.weight', 'encoder.layers.1.self_attn_layer_norm.bias', 'encoder.layers.11.self_attn_layer_norm.weight', 'encoder.layers.4.self_attn.k_proj.weight', 'encoder.layers.10.fc2.bias', 'encoder.layers.10.self_attn.k_proj.bias', 'encoder.layers.5.self_attn.v_proj.bias', 'encoder.layers.4.self_attn_layer_norm.bias', 'encoder.layers.5.self_attn.k_proj.bias', 'encoder.layers.2.fc1.weight', 'encoder.layers.10.fc1.weight', 'encoder.layers.7.self_attn.out_proj.weight', 'encoder.layers.3.self_attn.k_proj.bias', 'encoder.layers.3.fc1.weight', 'encoder.layers.10.self_attn.out_proj.bias', 'encoder.layers.8.self_attn.out_proj.weight', 'encoder.layers.11.self_attn.k_proj.bias', 'encoder.layers.0.self_attn.q_proj.weight', 'encoder.layers.6.self_attn.out_proj.bias', 'encoder.layers.1.self_attn.v_proj.weight', 'encoder.layers.2.fc2.weight', 'encoder.layers.11.fc1.bias', 'encoder.layers.3.self_attn.out_proj.weight', 'encoder.layers.3.fc2.weight', 'encoder.layernorm_embedding.bias', 'encoder.layers.0.self_attn.out_proj.weight', 'encoder.layers.7.self_attn.v_proj.weight', 'encoder.layers.7.self_attn.out_proj.bias', 'encoder.layers.2.final_layer_norm.bias', 'encoder.layers.11.self_attn.v_proj.bias', 'encoder.layers.9.fc1.bias', 'encoder.layers.4.fc1.bias', 'encoder.layers.2.self_attn.k_proj.bias', 'encoder.layers.10.final_layer_norm.weight', 'encoder.layers.10.self_attn.q_proj.bias', 'encoder.layers.7.self_attn.v_proj.bias', 'encoder.layers.1.self_attn.q_proj.bias', 'encoder.layers.7.fc1.weight', 'encoder.layers.3.self_attn_layer_norm.bias', 'shared.weight', 'encoder.layers.8.fc2.weight', 'encoder.layers.0.self_attn.k_proj.weight', 'encoder.layers.0.self_attn.k_proj.bias', 'encoder.layers.8.self_attn.k_proj.bias', 'encoder.layers.10.self_attn.out_proj.weight', 'encoder.layers.9.self_attn.q_proj.bias', 'encoder.layers.10.self_attn.v_proj.weight', 'encoder.layers.5.self_attn_layer_norm.weight', 'encoder.layers.4.fc1.weight', 'encoder.layers.2.fc1.bias', 'encoder.layers.8.fc1.weight', 'encoder.layers.9.self_attn.out_proj.bias', 'encoder.layers.9.fc2.bias', 'encoder.layers.9.final_layer_norm.bias', 'encoder.layers.5.self_attn.k_proj.weight', 'encoder.layers.8.final_layer_norm.weight', 'encoder.layers.7.self_attn.k_proj.weight', 'encoder.layers.5.final_layer_norm.weight', 'encoder.layers.11.fc2.weight', 'encoder.layers.1.self_attn_layer_norm.weight', 'encoder.layers.7.self_attn.q_proj.bias', 'encoder.layers.5.self_attn.v_proj.weight', 'encoder.layers.6.self_attn_layer_norm.bias', 'encoder.layers.10.final_layer_norm.bias', 'encoder.layers.10.self_attn.k_proj.weight', 'encoder.layers.4.self_attn.v_proj.bias', 'encoder.layers.9.self_attn.q_proj.weight', 'encoder.layers.7.final_layer_norm.weight', 'encoder.layers.11.final_layer_norm.weight', 'encoder.layers.3.self_attn.q_proj.weight', 'encoder.layers.6.final_layer_norm.bias', 'encoder.layers.0.final_layer_norm.weight', 'encoder.layers.3.self_attn.q_proj.bias', 'encoder.layers.5.self_attn.out_proj.weight', 'encoder.layers.3.self_attn.v_proj.bias', 'encoder.layers.11.self_attn.v_proj.weight', 'encoder.layers.2.self_attn.q_proj.bias', 'encoder.layers.0.fc1.bias', 'encoder.layers.6.self_attn.q_proj.weight', 'encoder.layers.3.self_attn.k_proj.weight', 'encoder.layers.2.self_attn.q_proj.weight', 'encoder.layers.1.fc2.bias', 'encoder.layers.6.self_attn.v_proj.weight', 'encoder.layers.2.final_layer_norm.weight', 'encoder.layers.1.self_attn.q_proj.weight', 'encoder.layers.8.final_layer_norm.bias', 'encoder.layers.2.self_attn.v_proj.bias', 'encoder.layers.0.fc2.bias', 'encoder.layers.5.self_attn_layer_norm.bias', 'encoder.layers.3.final_layer_norm.weight', 'encoder.layers.2.self_attn.out_proj.bias']\n",
      "- This IS expected if you are initializing BartForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BartForCausalLM were not initialized from the model checkpoint at facebook/bart-large and are newly initialized: ['lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    AutoModel\n",
    ")\n",
    "\n",
    "bart_tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large\")\n",
    "causal_prior_model = AutoModelForCausalLM.from_pretrained(\"facebook/bart-large\")\n",
    "bart_model = AutoModel.from_pretrained('facebook/bart-large')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load ENTFA data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data_utils import load_EntFA\n",
    "\n",
    "data_entfa = load_EntFA(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset xsum (/Users/anton164/.cache/huggingface/datasets/xsum/default/1.2.0/32c23220eadddb1149b16ed2e9430a05293768cfffbdfd151058697d4c11f934)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4ccff9061ed439a957c3cdf3fcc3d5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is Xsum doc equal to enfa source? False\n",
      "...with replaced newlines? True\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "xsum_test = load_dataset(\"xsum\")[\"test\"]\n",
    "xsum_test_by_id = {\n",
    "    doc[\"id\"]: doc for doc in xsum_test\n",
    "}\n",
    "\n",
    "# Verify that doc in dataset is equal to XSUM doc\n",
    "print(\n",
    "    \"Is Xsum doc equal to enfa source?\", xsum_test_by_id[\"37989821\"][\"document\"] == data_entfa[0][\"source\"]\n",
    ")\n",
    "print(\n",
    "    \"...with replaced newlines?\", xsum_test_by_id[\"37989821\"][\"document\"].replace(\"\\n\", \" \") == data_entfa[0][\"source\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's only equal if we replace newlines.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Sydney has marked the first anniversary of the siege at the Waverley cafe in which two women were killed by a gunman in the Australian city.',\n",
       " [{'start': 0,\n",
       "   'end': 6,\n",
       "   'label': 'Non-hallucinated',\n",
       "   'type': 'ORG',\n",
       "   'ent': 'Sydney'},\n",
       "  {'start': 22,\n",
       "   'end': 27,\n",
       "   'label': 'Non-hallucinated',\n",
       "   'type': 'ORDINAL',\n",
       "   'ent': 'first'},\n",
       "  {'start': 60,\n",
       "   'end': 68,\n",
       "   'label': 'Non-factual Hallucination',\n",
       "   'type': 'ORG',\n",
       "   'ent': 'Waverley'},\n",
       "  {'start': 83,\n",
       "   'end': 86,\n",
       "   'label': 'Non-hallucinated',\n",
       "   'type': 'CARDINAL',\n",
       "   'ent': 'two'},\n",
       "  {'start': 124,\n",
       "   'end': 134,\n",
       "   'label': 'Non-hallucinated',\n",
       "   'type': 'NORP',\n",
       "   'ent': 'Australian'}])"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.data_utils import lookup_entfa_by_prediction\n",
    "\n",
    "entfa_sydney = lookup_entfa_by_prediction(\n",
    "    data_entfa,\n",
    "    \"Sydney has marked the first anniversary of the siege at the Waverley cafe in which two women were killed by a gunman in the Australian city.\"\n",
    ")\n",
    "entfa_sydney[\"prediction\"], entfa_sydney[\"entities\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prior Prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('<s>', 0, 2.1549179541474617e-12),\n",
       " ('S', 104, 1.780824277375359e-06),\n",
       " ('yd', 9611, 2.7244109332968947e-06),\n",
       " ('ney', 2596, 3.707136642105979e-08),\n",
       " (' has', 34, 0.027413560077548027),\n",
       " (' marked', 4760, 1.1275883480266202e-06),\n",
       " (' the', 5, 0.00018941506277769804),\n",
       " (' first', 78, 0.0023676673881709576),\n",
       " (' anniversary', 4038, 5.814571068185614e-06),\n",
       " (' of', 9, 0.0017079596873372793),\n",
       " (' the', 5, 0.003059038193896413),\n",
       " (' siege', 19951, 3.6858385787930104e-10),\n",
       " (' at', 23, 0.0022077469620853662),\n",
       " (' the', 5, 0.0014361185021698475),\n",
       " (' W', 305, 0.002821172820404172),\n",
       " ('aver', 9903, 3.773652963445784e-07),\n",
       " ('ley', 607, 7.092088338822577e-08),\n",
       " (' cafe', 16381, 1.8298884185696807e-07),\n",
       " (' in', 11, 0.03579285740852356),\n",
       " (' which', 61, 2.48848186856776e-06),\n",
       " (' two', 80, 4.862476998823695e-05),\n",
       " (' women', 390, 0.00013294632663019001),\n",
       " (' were', 58, 0.00033202560734935105),\n",
       " (' killed', 848, 4.035213407860283e-08),\n",
       " (' by', 30, 4.279447966837324e-05),\n",
       " (' a', 10, 0.005261316429823637),\n",
       " (' gunman', 8928, 7.985426209167201e-11),\n",
       " (' in', 11, 0.0024648651015013456),\n",
       " (' the', 5, 0.001587406499311328),\n",
       " (' Australian', 2059, 1.335711263816819e-10),\n",
       " (' city', 343, 6.146278224150592e-07),\n",
       " ('.', 4, 3.3016669931384968e-06),\n",
       " ('</s>', 2, 1.8224537825517473e-06)]"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.masked_probability import prior_causal_probability\n",
    "prior_causal_probability(\n",
    "    causal_prior_model,\n",
    "    bart_tokenizer,\n",
    "    entfa_sydney[\"prediction\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Masked Prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForMaskedLM\n",
    "bart_tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large\")\n",
    "bart_masked = AutoModelForMaskedLM.from_pretrained(\"facebook/bart-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([{'masked_prob': tensor(0.0035),\n",
       "   'input': '<s>Sydney has marked the first anniversary of the siege at the<mask>',\n",
       "   'top_probs': [' Sydney',\n",
       "    ' siege',\n",
       "    ' Port',\n",
       "    ' Siege',\n",
       "    ' end',\n",
       "    ' Australian',\n",
       "    ' Hilton',\n",
       "    ' NSW',\n",
       "    ' Red',\n",
       "    ' Royal']},\n",
       "  {'masked_prob': tensor(1.7951e-05),\n",
       "   'input': '<s>Sydney has marked the first anniversary of the siege at the W<mask>',\n",
       "   'top_probs': [' Hotel',\n",
       "    ' Building',\n",
       "    ' Block',\n",
       "    ' W',\n",
       "    ' Bank',\n",
       "    ' and',\n",
       "    ' in',\n",
       "    ' for',\n",
       "    ' Club',\n",
       "    ' on']},\n",
       "  {'masked_prob': tensor(0.0003),\n",
       "   'input': '<s>Sydney has marked the first anniversary of the siege at the Waver<mask>',\n",
       "   'top_probs': [' Beach',\n",
       "    ' Island',\n",
       "    ' Creek',\n",
       "    ' Point',\n",
       "    ' Hill',\n",
       "    ' House',\n",
       "    ' Arms',\n",
       "    ' Farm',\n",
       "    ' Bay',\n",
       "    ' Cove']}],\n",
       " tensor(2.0389e-11))"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.masked_probability import prior_masked_probability\n",
    "prior_masked_probability(\n",
    "    bart_masked, \n",
    "    bart_tokenizer, \n",
    "    \"Sydney has marked the first anniversary of the siege at the\", \n",
    "    \" Waverley\"\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([{'masked_prob': tensor(0.0385),\n",
       "   'input': '<s>Sydney has marked the<mask>',\n",
       "   'top_probs': [' start',\n",
       "    ' cent',\n",
       "    ' end',\n",
       "    ' 100',\n",
       "    ' 50',\n",
       "    ' first',\n",
       "    ' 20',\n",
       "    ' 10',\n",
       "    ' one',\n",
       "    ' 25']}],\n",
       " tensor(0.0385))"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prior_masked_probability(\n",
    "    bart_masked, \n",
    "    bart_tokenizer, \n",
    "    \"Sydney has marked the\", \n",
    "    \" first\"\n",
    ") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Posterior Prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoModelForSeq2SeqLM\n",
    ")\n",
    "\n",
    "bart_xsum = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/bart-large-xsum\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What would bart-xsum have generated?\n",
    "\n",
    "With and without replacing newline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Sydney has marked the first anniversary of the siege at the Waverley cafe in which two people were killed.',\n",
       " 'Sydney has marked the first anniversary of the siege at the Waverley cafe in which two women were killed.']"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.generation_utils import generate_summaries\n",
    "doc = xsum_test_by_id[\"35099282\"][\"document\"]\n",
    "docs_to_summarize = [\n",
    "    doc, \n",
    "    doc.replace(\"\\n\", \" \")\n",
    "]\n",
    "\n",
    "sums = generate_summaries(\n",
    "    bart_xsum,\n",
    "    bart_tokenizer,\n",
    "    docs_to_summarize,\n",
    "    None,\n",
    "    num_beams=4\n",
    ")\n",
    "sums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sydney has marked the first anniversary of the siege at the Waverley cafe in which two women were killed by a gunman in the Australian city.'"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entfa_sydney[\"prediction\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('S', 104, tensor([0.1869, 0.2390])),\n",
       " ('yd', 9611, tensor([0.9439, 0.9353])),\n",
       " ('ney', 2596, tensor([0.9553, 0.9534])),\n",
       " (' has', 34, tensor([0.5095, 0.5292])),\n",
       " (' marked', 4760, tensor([0.3043, 0.2741])),\n",
       " (' the', 5, tensor([0.6830, 0.7088])),\n",
       " (' first', 78, tensor([0.6297, 0.6220])),\n",
       " (' anniversary', 4038, tensor([0.9380, 0.9416])),\n",
       " (' of', 9, tensor([0.8882, 0.8881])),\n",
       " (' the', 5, tensor([0.8174, 0.8127])),\n",
       " (' siege', 19951, tensor([0.3841, 0.3650])),\n",
       " (' at', 23, tensor([0.7233, 0.7404])),\n",
       " (' the', 5, tensor([0.5626, 0.5693])),\n",
       " (' W', 305, tensor([0.0992, 0.1038])),\n",
       " ('aver', 9903, tensor([0.7166, 0.6745])),\n",
       " ('ley', 607, tensor([0.9400, 0.9403])),\n",
       " (' cafe', 16381, tensor([0.5410, 0.5290])),\n",
       " (' in', 11, tensor([0.3207, 0.3224])),\n",
       " (' which', 61, tensor([0.7555, 0.8088])),\n",
       " (' two', 80, tensor([0.6819, 0.7448])),\n",
       " (' women', 390, tensor([0.3642, 0.3930])),\n",
       " (' were', 58, tensor([0.6122, 0.6101])),\n",
       " (' killed', 848, tensor([0.8426, 0.8378])),\n",
       " (' by', 30, tensor([0.3655, 0.3821])),\n",
       " (' a', 10, tensor([0.1983, 0.2129])),\n",
       " (' gunman', 8928, tensor([0.9064, 0.8995])),\n",
       " (' in', 11, tensor([0.0278, 0.0273])),\n",
       " (' the', 5, tensor([0.3501, 0.2526])),\n",
       " (' Australian', 2059, tensor([0.0651, 0.0572])),\n",
       " (' city', 343, tensor([0.6193, 0.6226])),\n",
       " ('.', 4, tensor([0.8387, 0.8374])),\n",
       " ('</s>', 2, tensor([0.8844, 0.8984]))]"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.masked_probability import forceful_conditional_generation\n",
    "preds, sums = forceful_conditional_generation(\n",
    "    bart_xsum,\n",
    "    bart_tokenizer,\n",
    "    entfa_sydney[\"prediction\"],\n",
    "    docs_to_summarize\n",
    ")\n",
    "preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 <s>\n",
      "1 104 S\n",
      "2 9611 yd\n",
      "3 2596 ney\n",
      "4 34  has\n",
      "5 4760  marked\n",
      "6 5  the\n",
      "7 78  first\n",
      "8 4038  anniversary\n",
      "9 9  of\n"
     ]
    }
   ],
   "source": [
    "model = causal_prior_model\n",
    "model.eval()\n",
    "tokenizer = bart_tokenizer\n",
    "tokenized = tokenizer.encode(\n",
    "    entfa_sydney[\"prediction\"], \n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "for j, t in list(enumerate(tokenized[0]))[:10]:\n",
    "    print(j, t.item(), tokenizer.decode(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   0,  104, 9611]])"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = tokenized[:, :3]\n",
    "input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 50265])"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = model(input).logits[0]\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50265])\n",
      "torch.return_types.topk(\n",
      "values=tensor([0.5963, 0.2194, 0.0257], grad_fn=<TopkBackward0>),\n",
      "indices=tensor([36567, 12591, 34479]))\n",
      "torch.Size([50265])\n",
      "torch.return_types.topk(\n",
      "values=tensor([0.4562, 0.3828, 0.0283], grad_fn=<TopkBackward0>),\n",
      "indices=tensor([36567, 12591,  4450]))\n",
      "torch.Size([50265])\n",
      "torch.return_types.topk(\n",
      "values=tensor([0.0672, 0.0656, 0.0545], grad_fn=<TopkBackward0>),\n",
      "indices=tensor([  287,  3644, 12921]))\n"
     ]
    }
   ],
   "source": [
    "input = tokenized[:, :3]\n",
    "output = model(input).logits[0]\n",
    "\n",
    "for dist in output.softmax(dim=1):\n",
    "    print(dist.shape)\n",
    "    print(dist.topk(k=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50265])\n",
      "torch.return_types.topk(\n",
      "values=tensor([0.5963, 0.2194, 0.0257]),\n",
      "indices=tensor([36567, 12591, 34479]))\n",
      "torch.Size([50265])\n",
      "torch.return_types.topk(\n",
      "values=tensor([0.4562, 0.3828, 0.0283]),\n",
      "indices=tensor([36567, 12591,  4450]))\n",
      "torch.Size([50265])\n",
      "torch.return_types.topk(\n",
      "values=tensor([0.0672, 0.0656, 0.0545]),\n",
      "indices=tensor([  287,  3644, 12921]))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "with torch.no_grad():\n",
    "    input = tokenized[:, :3]\n",
    "    output = model(input).logits[0]\n",
    "\n",
    "    for dist in output.softmax(dim=1):\n",
    "        print(dist.shape)\n",
    "        print(dist.topk(k=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5962832570075989 aida 36567\n",
      "0.21944782137870789  Edge 12591\n",
      "0.025743484497070312  ESA 34479\n"
     ]
    }
   ],
   "source": [
    "top = output.softmax(dim=1)[0].topk(k=3)\n",
    "\n",
    "for prob, tok in zip(top.values, top.indices):\n",
    "    print(prob.item(), tokenizer.decode(tok), tok.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    BartForConditionalGeneration,\n",
    ")\n",
    "\n",
    "model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('<s>', 0, 1.609744504094124e-05, tensor([  104,   133, 25533])),\n",
       " ('S', 104, 0.994608998298645, tensor([  104,   133, 25533])),\n",
       " ('yd', 9611, 0.9998235106468201, tensor([9611, 2981, 4183])),\n",
       " ('ney', 2596, 0.9999237060546875, tensor([ 2596, 30915, 17251])),\n",
       " (' has', 34, 0.7072009444236755, tensor([   34,  4863, 16381])),\n",
       " (' marked', 4760, 0.9997780919075012, tensor([ 4760, 10032, 16293])),\n",
       " (' the', 5, 0.9999321699142456, tensor([  5,  63, 133])),\n",
       " (' first', 78, 0.9972997307777405, tensor([ 78,  65, 112])),\n",
       " (' anniversary', 4038, 0.9999219179153442, tensor([4038,   76,  183])),\n",
       " (' of', 9, 0.9997175335884094, tensor([ 9, 19,  6])),\n",
       " (' the', 5, 0.999014139175415, tensor([ 5, 10, 41])),\n",
       " (' siege', 19951, 0.9994810223579407, tensor([19951,   305, 16381])),\n",
       " (' at', 23, 0.9993714690208435, tensor([  23, 3750,   11])),\n",
       " (' the', 5, 0.9999973773956299, tensor([  5,  10, 305])),\n",
       " (' W', 305, 0.9999921321868896, tensor([  305,   580, 16381])),\n",
       " ('aver', 9903, 0.9999996423721313, tensor([ 9903, 19909,  1469])),\n",
       " ('ley', 607, 0.9999991655349731, tensor([ 607,  352, 5610])),\n",
       " (' cafe', 16381, 0.9999502897262573, tensor([16381, 16542, 19951])),\n",
       " (' in', 11, 0.9997674822807312, tensor([  11, 1121,   61])),\n",
       " (' which', 61, 0.9999828338623047, tensor([   61, 16773,  1014])),\n",
       " (' two', 80, 0.9999988079071045, tensor([ 80, 249, 130])),\n",
       " (' women', 390, 0.999998927116394, tensor([390,  82, 664])),\n",
       " (' were', 58, 0.9999984502792358, tensor([58, 32,  8])),\n",
       " (' killed', 848, 0.9999996423721313, tensor([ 848,  738, 1462])),\n",
       " (' by', 30, 0.9999994039535522, tensor([30, 11, 23])),\n",
       " (' a', 10, 1.0, tensor([10, 41, 83])),\n",
       " (' gunman', 8928, 0.9999998807907104, tensor([ 8928, 18282,  1751])),\n",
       " (' in', 11, 0.999992847442627, tensor([  11, 1121,   23])),\n",
       " (' the', 5, 0.9999997615814209, tensor([ 5, 41, 10])),\n",
       " (' Australian', 2059, 0.9999898672103882, tensor([2059, 1221, 2027])),\n",
       " (' city', 343, 0.999994158744812, tensor([ 343,  812, 1947])),\n",
       " ('.', 4, 1.0, tensor([    4,     6, 16381])),\n",
       " ('</s>', 2, 0.9685935378074646, tensor([ 2, 20, 83]))]"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = 'Sydney has marked the first anniversary of the siege at the Waverley cafe in which two women were killed by a gunman in the Australian city.'\n",
    "\n",
    "with torch.no_grad():\n",
    "    target_tokens = tokenizer(target, return_tensors=\"pt\").input_ids\n",
    "\n",
    "    logits = model(target_tokens).logits\n",
    "    probs = logits.squeeze(0).softmax(dim=1)\n",
    "\n",
    "    preds = []\n",
    "    for step_probs, target_token in zip(probs, target_tokens[0]):\n",
    "        preds.append(\n",
    "            (\n",
    "                tokenizer.decode(target_token),\n",
    "                target_token.item(),\n",
    "                step_probs[target_token].item(),\n",
    "                step_probs.topk(k=3).indices\n",
    "            )\n",
    "        )\n",
    "\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('<s>', 0, 2.1549179541474617e-12),\n",
       " ('S', 104, 1.780824277375359e-06),\n",
       " ('yd', 9611, 2.7244109332968947e-06),\n",
       " ('ney', 2596, 3.707136642105979e-08),\n",
       " (' has', 34, 0.027413560077548027),\n",
       " (' marked', 4760, 1.1275883480266202e-06),\n",
       " (' the', 5, 0.00018941506277769804),\n",
       " (' first', 78, 0.0023676673881709576),\n",
       " (' anniversary', 4038, 5.814571068185614e-06),\n",
       " (' of', 9, 0.0017079596873372793),\n",
       " (' the', 5, 0.003059038193896413),\n",
       " (' siege', 19951, 3.6858385787930104e-10),\n",
       " (' at', 23, 0.0022077469620853662),\n",
       " (' the', 5, 0.0014361185021698475),\n",
       " (' W', 305, 0.002821172820404172),\n",
       " ('aver', 9903, 3.773652963445784e-07),\n",
       " ('ley', 607, 7.092088338822577e-08),\n",
       " (' cafe', 16381, 1.8298884185696807e-07),\n",
       " (' in', 11, 0.03579285740852356),\n",
       " (' which', 61, 2.48848186856776e-06),\n",
       " (' two', 80, 4.862476998823695e-05),\n",
       " (' women', 390, 0.00013294632663019001),\n",
       " (' were', 58, 0.00033202560734935105),\n",
       " (' killed', 848, 4.035213407860283e-08),\n",
       " (' by', 30, 4.279447966837324e-05),\n",
       " (' a', 10, 0.005261316429823637),\n",
       " (' gunman', 8928, 7.985426209167201e-11),\n",
       " (' in', 11, 0.0024648651015013456),\n",
       " (' the', 5, 0.001587406499311328),\n",
       " (' Australian', 2059, 1.335711263816819e-10),\n",
       " (' city', 343, 6.146278224150592e-07),\n",
       " ('.', 4, 3.3016669931384968e-06),\n",
       " ('</s>', 2, 1.8224537825517473e-06)]"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3f3fe5eacab954956417c9014984aac9d559687fff881692b0d4e46fa0e895bd"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('factual-beam-search')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
