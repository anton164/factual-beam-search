{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prior prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/bart-large were not used when initializing BartForCausalLM: ['encoder.layers.0.fc2.weight', 'encoder.layers.1.self_attn.out_proj.bias', 'encoder.layers.4.self_attn.out_proj.bias', 'encoder.layers.8.self_attn.q_proj.weight', 'encoder.layers.10.self_attn_layer_norm.weight', 'encoder.layers.0.final_layer_norm.bias', 'encoder.layers.4.fc2.bias', 'encoder.layers.4.final_layer_norm.bias', 'encoder.layers.7.fc1.bias', 'encoder.layers.4.self_attn.q_proj.weight', 'encoder.layers.9.fc1.weight', 'encoder.layers.5.self_attn.out_proj.bias', 'encoder.layers.6.self_attn.out_proj.weight', 'encoder.layers.1.fc1.bias', 'encoder.layers.5.self_attn.q_proj.bias', 'encoder.layers.4.self_attn.v_proj.weight', 'encoder.layers.10.fc2.weight', 'encoder.layers.10.self_attn_layer_norm.bias', 'encoder.layers.6.self_attn.k_proj.bias', 'encoder.layers.3.self_attn.out_proj.bias', 'encoder.layers.4.self_attn.q_proj.bias', 'encoder.layers.5.self_attn.q_proj.weight', 'encoder.layers.6.fc1.bias', 'encoder.layers.7.fc2.weight', 'encoder.layers.4.final_layer_norm.weight', 'encoder.layers.8.self_attn_layer_norm.weight', 'encoder.layers.6.fc1.weight', 'encoder.layers.9.final_layer_norm.weight', 'encoder.layers.11.self_attn.k_proj.weight', 'encoder.layers.7.self_attn.k_proj.bias', 'encoder.layers.9.self_attn_layer_norm.weight', 'encoder.layers.8.self_attn_layer_norm.bias', 'encoder.layers.2.self_attn.k_proj.weight', 'encoder.layers.9.self_attn_layer_norm.bias', 'encoder.layers.5.fc2.weight', 'encoder.layers.10.self_attn.v_proj.bias', 'encoder.layers.0.self_attn_layer_norm.weight', 'encoder.layers.1.final_layer_norm.weight', 'encoder.layers.1.fc1.weight', 'encoder.layers.6.fc2.bias', 'encoder.layernorm_embedding.weight', 'encoder.layers.11.self_attn.q_proj.weight', 'encoder.layers.6.fc2.weight', 'encoder.layers.11.final_layer_norm.bias', 'encoder.layers.1.final_layer_norm.bias', 'encoder.layers.7.self_attn_layer_norm.weight', 'encoder.layers.1.self_attn.v_proj.bias', 'encoder.layers.6.self_attn.v_proj.bias', 'encoder.layers.0.self_attn.v_proj.bias', 'encoder.layers.0.self_attn.out_proj.bias', 'encoder.layers.11.self_attn.out_proj.bias', 'encoder.layers.10.fc1.bias', 'encoder.layers.11.self_attn_layer_norm.bias', 'encoder.layers.0.fc1.weight', 'encoder.layers.9.self_attn.out_proj.weight', 'encoder.layers.8.self_attn.k_proj.weight', 'encoder.layers.11.fc2.bias', 'encoder.layers.6.self_attn_layer_norm.weight', 'encoder.layers.3.final_layer_norm.bias', 'encoder.layers.4.self_attn_layer_norm.weight', 'encoder.layers.11.self_attn.out_proj.weight', 'encoder.layers.5.final_layer_norm.bias', 'encoder.layers.8.self_attn.v_proj.weight', 'encoder.layers.9.self_attn.v_proj.bias', 'encoder.layers.8.fc1.bias', 'encoder.layers.10.self_attn.q_proj.weight', 'encoder.layers.1.self_attn.k_proj.bias', 'encoder.layers.2.self_attn_layer_norm.bias', 'encoder.layers.8.self_attn.out_proj.bias', 'encoder.layers.8.self_attn.q_proj.bias', 'encoder.layers.7.self_attn_layer_norm.bias', 'encoder.layers.0.self_attn_layer_norm.bias', 'encoder.layers.2.self_attn.v_proj.weight', 'encoder.layers.9.self_attn.k_proj.bias', 'encoder.layers.11.self_attn.q_proj.bias', 'encoder.layers.4.fc2.weight', 'encoder.layers.2.self_attn.out_proj.weight', 'encoder.layers.9.self_attn.v_proj.weight', 'encoder.layers.5.fc1.weight', 'encoder.layers.1.self_attn.k_proj.weight', 'encoder.layers.3.fc1.bias', 'encoder.layers.0.self_attn.q_proj.bias', 'encoder.layers.11.fc1.weight', 'encoder.layers.1.self_attn.out_proj.weight', 'encoder.embed_positions.weight', 'encoder.layers.6.final_layer_norm.weight', 'encoder.layers.3.self_attn.v_proj.weight', 'encoder.layers.2.fc2.bias', 'encoder.layers.1.fc2.weight', 'encoder.layers.3.fc2.bias', 'encoder.layers.2.self_attn_layer_norm.weight', 'encoder.layers.7.self_attn.q_proj.weight', 'encoder.layers.4.self_attn.out_proj.weight', 'encoder.layers.8.fc2.bias', 'encoder.layers.8.self_attn.v_proj.bias', 'encoder.layers.4.self_attn.k_proj.bias', 'encoder.layers.7.fc2.bias', 'encoder.layers.5.fc2.bias', 'encoder.layers.0.self_attn.v_proj.weight', 'encoder.layers.7.final_layer_norm.bias', 'encoder.layers.6.self_attn.k_proj.weight', 'encoder.layers.9.self_attn.k_proj.weight', 'encoder.layers.3.self_attn_layer_norm.weight', 'encoder.layers.9.fc2.weight', 'encoder.layers.6.self_attn.q_proj.bias', 'encoder.layers.5.fc1.bias', 'encoder.embed_tokens.weight', 'encoder.layers.1.self_attn_layer_norm.bias', 'encoder.layers.11.self_attn_layer_norm.weight', 'encoder.layers.4.self_attn.k_proj.weight', 'encoder.layers.10.fc2.bias', 'encoder.layers.10.self_attn.k_proj.bias', 'encoder.layers.5.self_attn.v_proj.bias', 'encoder.layers.4.self_attn_layer_norm.bias', 'encoder.layers.5.self_attn.k_proj.bias', 'encoder.layers.2.fc1.weight', 'encoder.layers.10.fc1.weight', 'encoder.layers.7.self_attn.out_proj.weight', 'encoder.layers.3.self_attn.k_proj.bias', 'encoder.layers.3.fc1.weight', 'encoder.layers.10.self_attn.out_proj.bias', 'encoder.layers.8.self_attn.out_proj.weight', 'encoder.layers.11.self_attn.k_proj.bias', 'encoder.layers.0.self_attn.q_proj.weight', 'encoder.layers.6.self_attn.out_proj.bias', 'encoder.layers.1.self_attn.v_proj.weight', 'encoder.layers.2.fc2.weight', 'encoder.layers.11.fc1.bias', 'encoder.layers.3.self_attn.out_proj.weight', 'encoder.layers.3.fc2.weight', 'encoder.layernorm_embedding.bias', 'encoder.layers.0.self_attn.out_proj.weight', 'encoder.layers.7.self_attn.v_proj.weight', 'encoder.layers.7.self_attn.out_proj.bias', 'encoder.layers.2.final_layer_norm.bias', 'encoder.layers.11.self_attn.v_proj.bias', 'encoder.layers.9.fc1.bias', 'encoder.layers.4.fc1.bias', 'encoder.layers.2.self_attn.k_proj.bias', 'encoder.layers.10.final_layer_norm.weight', 'encoder.layers.10.self_attn.q_proj.bias', 'encoder.layers.7.self_attn.v_proj.bias', 'encoder.layers.1.self_attn.q_proj.bias', 'encoder.layers.7.fc1.weight', 'encoder.layers.3.self_attn_layer_norm.bias', 'shared.weight', 'encoder.layers.8.fc2.weight', 'encoder.layers.0.self_attn.k_proj.weight', 'encoder.layers.0.self_attn.k_proj.bias', 'encoder.layers.8.self_attn.k_proj.bias', 'encoder.layers.10.self_attn.out_proj.weight', 'encoder.layers.9.self_attn.q_proj.bias', 'encoder.layers.10.self_attn.v_proj.weight', 'encoder.layers.5.self_attn_layer_norm.weight', 'encoder.layers.4.fc1.weight', 'encoder.layers.2.fc1.bias', 'encoder.layers.8.fc1.weight', 'encoder.layers.9.self_attn.out_proj.bias', 'encoder.layers.9.fc2.bias', 'encoder.layers.9.final_layer_norm.bias', 'encoder.layers.5.self_attn.k_proj.weight', 'encoder.layers.8.final_layer_norm.weight', 'encoder.layers.7.self_attn.k_proj.weight', 'encoder.layers.5.final_layer_norm.weight', 'encoder.layers.11.fc2.weight', 'encoder.layers.1.self_attn_layer_norm.weight', 'encoder.layers.7.self_attn.q_proj.bias', 'encoder.layers.5.self_attn.v_proj.weight', 'encoder.layers.6.self_attn_layer_norm.bias', 'encoder.layers.10.final_layer_norm.bias', 'encoder.layers.10.self_attn.k_proj.weight', 'encoder.layers.4.self_attn.v_proj.bias', 'encoder.layers.9.self_attn.q_proj.weight', 'encoder.layers.7.final_layer_norm.weight', 'encoder.layers.11.final_layer_norm.weight', 'encoder.layers.3.self_attn.q_proj.weight', 'encoder.layers.6.final_layer_norm.bias', 'encoder.layers.0.final_layer_norm.weight', 'encoder.layers.3.self_attn.q_proj.bias', 'encoder.layers.5.self_attn.out_proj.weight', 'encoder.layers.3.self_attn.v_proj.bias', 'encoder.layers.11.self_attn.v_proj.weight', 'encoder.layers.2.self_attn.q_proj.bias', 'encoder.layers.0.fc1.bias', 'encoder.layers.6.self_attn.q_proj.weight', 'encoder.layers.3.self_attn.k_proj.weight', 'encoder.layers.2.self_attn.q_proj.weight', 'encoder.layers.1.fc2.bias', 'encoder.layers.6.self_attn.v_proj.weight', 'encoder.layers.2.final_layer_norm.weight', 'encoder.layers.1.self_attn.q_proj.weight', 'encoder.layers.8.final_layer_norm.bias', 'encoder.layers.2.self_attn.v_proj.bias', 'encoder.layers.0.fc2.bias', 'encoder.layers.5.self_attn_layer_norm.bias', 'encoder.layers.3.final_layer_norm.weight', 'encoder.layers.2.self_attn.out_proj.bias']\n",
      "- This IS expected if you are initializing BartForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BartForCausalLM were not initialized from the model checkpoint at facebook/bart-large and are newly initialized: ['lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    AutoModel\n",
    ")\n",
    "\n",
    "bart_tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large\")\n",
    "causal_prior_model = AutoModelForCausalLM.from_pretrained(\"facebook/bart-large\")\n",
    "bart_model = AutoModel.from_pretrained('facebook/bart-large')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load ENTFA data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data_utils import load_EntFA\n",
    "\n",
    "data_entfa = load_EntFA(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset xsum (/Users/anton164/.cache/huggingface/datasets/xsum/default/1.2.0/32c23220eadddb1149b16ed2e9430a05293768cfffbdfd151058697d4c11f934)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4ccff9061ed439a957c3cdf3fcc3d5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is Xsum doc equal to enfa source? False\n",
      "...with replaced newlines? True\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "xsum_test = load_dataset(\"xsum\")[\"test\"]\n",
    "xsum_test_by_id = {\n",
    "    doc[\"id\"]: doc for doc in xsum_test\n",
    "}\n",
    "\n",
    "# Verify that doc in dataset is equal to XSUM doc\n",
    "print(\n",
    "    \"Is Xsum doc equal to enfa source?\", xsum_test_by_id[\"37989821\"][\"document\"] == data_entfa[0][\"source\"]\n",
    ")\n",
    "print(\n",
    "    \"...with replaced newlines?\", xsum_test_by_id[\"37989821\"][\"document\"].replace(\"\\n\", \" \") == data_entfa[0][\"source\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's only equal if we replace newlines.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Sydney has marked the first anniversary of the siege at the Waverley cafe in which two women were killed by a gunman in the Australian city.',\n",
       " [{'start': 0,\n",
       "   'end': 6,\n",
       "   'label': 'Non-hallucinated',\n",
       "   'type': 'ORG',\n",
       "   'ent': 'Sydney'},\n",
       "  {'start': 22,\n",
       "   'end': 27,\n",
       "   'label': 'Non-hallucinated',\n",
       "   'type': 'ORDINAL',\n",
       "   'ent': 'first'},\n",
       "  {'start': 60,\n",
       "   'end': 68,\n",
       "   'label': 'Non-factual Hallucination',\n",
       "   'type': 'ORG',\n",
       "   'ent': 'Waverley'},\n",
       "  {'start': 83,\n",
       "   'end': 86,\n",
       "   'label': 'Non-hallucinated',\n",
       "   'type': 'CARDINAL',\n",
       "   'ent': 'two'},\n",
       "  {'start': 124,\n",
       "   'end': 134,\n",
       "   'label': 'Non-hallucinated',\n",
       "   'type': 'NORP',\n",
       "   'ent': 'Australian'}])"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.data_utils import lookup_entfa_by_prediction\n",
    "\n",
    "entfa_sydney = lookup_entfa_by_prediction(\n",
    "    data_entfa,\n",
    "    \"Sydney has marked the first anniversary of the siege at the Waverley cafe in which two women were killed by a gunman in the Australian city.\"\n",
    ")\n",
    "entfa_sydney[\"prediction\"], entfa_sydney[\"entities\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prior Prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('<s>', 0, 2.1549179541474617e-12),\n",
       " ('S', 104, 1.780824277375359e-06),\n",
       " ('yd', 9611, 2.7244109332968947e-06),\n",
       " ('ney', 2596, 3.707136642105979e-08),\n",
       " (' has', 34, 0.027413560077548027),\n",
       " (' marked', 4760, 1.1275883480266202e-06),\n",
       " (' the', 5, 0.00018941506277769804),\n",
       " (' first', 78, 0.0023676673881709576),\n",
       " (' anniversary', 4038, 5.814571068185614e-06),\n",
       " (' of', 9, 0.0017079596873372793),\n",
       " (' the', 5, 0.003059038193896413),\n",
       " (' siege', 19951, 3.6858385787930104e-10),\n",
       " (' at', 23, 0.0022077469620853662),\n",
       " (' the', 5, 0.0014361185021698475),\n",
       " (' W', 305, 0.002821172820404172),\n",
       " ('aver', 9903, 3.773652963445784e-07),\n",
       " ('ley', 607, 7.092088338822577e-08),\n",
       " (' cafe', 16381, 1.8298884185696807e-07),\n",
       " (' in', 11, 0.03579285740852356),\n",
       " (' which', 61, 2.48848186856776e-06),\n",
       " (' two', 80, 4.862476998823695e-05),\n",
       " (' women', 390, 0.00013294632663019001),\n",
       " (' were', 58, 0.00033202560734935105),\n",
       " (' killed', 848, 4.035213407860283e-08),\n",
       " (' by', 30, 4.279447966837324e-05),\n",
       " (' a', 10, 0.005261316429823637),\n",
       " (' gunman', 8928, 7.985426209167201e-11),\n",
       " (' in', 11, 0.0024648651015013456),\n",
       " (' the', 5, 0.001587406499311328),\n",
       " (' Australian', 2059, 1.335711263816819e-10),\n",
       " (' city', 343, 6.146278224150592e-07),\n",
       " ('.', 4, 3.3016669931384968e-06),\n",
       " ('</s>', 2, 1.8224537825517473e-06)]"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.masked_probability import prior_causal_probability\n",
    "prior_causal_probability(\n",
    "    causal_prior_model,\n",
    "    bart_tokenizer,\n",
    "    entfa_sydney[\"prediction\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Posterior Prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoModelForSeq2SeqLM\n",
    ")\n",
    "\n",
    "bart_xsum = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/bart-large-xsum\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What would bart-xsum have generated?\n",
    "\n",
    "With and without replacing newline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Sydney has marked the first anniversary of the siege at the Waverley cafe in which two people were killed.',\n",
       " 'Sydney has marked the first anniversary of the siege at the Waverley cafe in which two women were killed.']"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.generation_utils import generate_summaries\n",
    "doc = xsum_test_by_id[\"35099282\"][\"document\"]\n",
    "docs_to_summarize = [\n",
    "    doc, \n",
    "    doc.replace(\"\\n\", \" \")\n",
    "]\n",
    "\n",
    "sums = generate_summaries(\n",
    "    bart_xsum,\n",
    "    bart_tokenizer,\n",
    "    docs_to_summarize,\n",
    "    None,\n",
    "    num_beams=4\n",
    ")\n",
    "sums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sydney has marked the first anniversary of the siege at the Waverley cafe in which two women were killed by a gunman in the Australian city.'"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entfa_sydney[\"prediction\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('S', 104, tensor([0.1869, 0.2390])),\n",
       " ('yd', 9611, tensor([0.9439, 0.9353])),\n",
       " ('ney', 2596, tensor([0.9553, 0.9534])),\n",
       " (' has', 34, tensor([0.5095, 0.5292])),\n",
       " (' marked', 4760, tensor([0.3043, 0.2741])),\n",
       " (' the', 5, tensor([0.6830, 0.7088])),\n",
       " (' first', 78, tensor([0.6297, 0.6220])),\n",
       " (' anniversary', 4038, tensor([0.9380, 0.9416])),\n",
       " (' of', 9, tensor([0.8882, 0.8881])),\n",
       " (' the', 5, tensor([0.8174, 0.8127])),\n",
       " (' siege', 19951, tensor([0.3841, 0.3650])),\n",
       " (' at', 23, tensor([0.7233, 0.7404])),\n",
       " (' the', 5, tensor([0.5626, 0.5693])),\n",
       " (' W', 305, tensor([0.0992, 0.1038])),\n",
       " ('aver', 9903, tensor([0.7166, 0.6745])),\n",
       " ('ley', 607, tensor([0.9400, 0.9403])),\n",
       " (' cafe', 16381, tensor([0.5410, 0.5290])),\n",
       " (' in', 11, tensor([0.3207, 0.3224])),\n",
       " (' which', 61, tensor([0.7555, 0.8088])),\n",
       " (' two', 80, tensor([0.6819, 0.7448])),\n",
       " (' women', 390, tensor([0.3642, 0.3930])),\n",
       " (' were', 58, tensor([0.6122, 0.6101])),\n",
       " (' killed', 848, tensor([0.8426, 0.8378])),\n",
       " (' by', 30, tensor([0.3655, 0.3821])),\n",
       " (' a', 10, tensor([0.1983, 0.2129])),\n",
       " (' gunman', 8928, tensor([0.9064, 0.8995])),\n",
       " (' in', 11, tensor([0.0278, 0.0273])),\n",
       " (' the', 5, tensor([0.3501, 0.2526])),\n",
       " (' Australian', 2059, tensor([0.0651, 0.0572])),\n",
       " (' city', 343, tensor([0.6193, 0.6226])),\n",
       " ('.', 4, tensor([0.8387, 0.8374])),\n",
       " ('</s>', 2, tensor([0.8844, 0.8984]))]"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.masked_probability import forceful_conditional_generation\n",
    "preds, sums = forceful_conditional_generation(\n",
    "    bart_xsum,\n",
    "    bart_tokenizer,\n",
    "    entfa_sydney[\"prediction\"],\n",
    "    docs_to_summarize\n",
    ")\n",
    "preds"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3f3fe5eacab954956417c9014984aac9d559687fff881692b0d4e46fa0e895bd"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('factual-beam-search')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
