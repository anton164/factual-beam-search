{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BART 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corrector is_factual: 0.42 (baseline diff: 0.0)\n",
      "corrector is_factual baseline diff mcnemar p-value: 1.0\n",
      "corrector is_factual baseline diff wilcoxon p-value: 1.0\n",
      "corrector has_extrinsic_and_fully_factual: 0.45 (baseline diff: 0.010000000000000009)\n",
      "corrector has_extrinsic_and_fully_factual baseline diff mcnemar p-value: 1.0\n",
      "corrector has_extrinsic_and_fully_factual baseline diff wilcoxon p-value: 0.7054569861112734\n",
      "\n",
      "pinocchio is_factual: 0.43 (baseline diff: 0.010000000000000009)\n",
      "pinocchio is_factual baseline diff mcnemar p-value: 1.0\n",
      "pinocchio is_factual baseline diff wilcoxon p-value: 0.5637028616507731\n",
      "pinocchio has_extrinsic_and_fully_factual: 0.43 (baseline diff: -0.010000000000000009)\n",
      "pinocchio has_extrinsic_and_fully_factual baseline diff mcnemar p-value: 1.0\n",
      "pinocchio has_extrinsic_and_fully_factual baseline diff wilcoxon p-value: 0.5637028616507731\n",
      "\n",
      "rl-fact is_factual: 0.55 (baseline diff: 0.13000000000000006)\n",
      "rl-fact is_factual baseline diff mcnemar p-value: 0.004425048828125\n",
      "rl-fact is_factual baseline diff wilcoxon p-value: 0.0028599382084640934\n",
      "rl-fact has_extrinsic_and_fully_factual: 0.39 (baseline diff: -0.04999999999999999)\n",
      "rl-fact has_extrinsic_and_fully_factual baseline diff mcnemar p-value: 0.359283447265625\n",
      "rl-fact has_extrinsic_and_fully_factual baseline diff wilcoxon p-value: 0.25134910881022265\n",
      "\n",
      "gef_classifier is_factual: 0.53 (baseline diff: 0.11000000000000004)\n",
      "gef_classifier is_factual baseline diff mcnemar p-value: 0.00341796875\n",
      "gef_classifier is_factual baseline diff wilcoxon p-value: 0.0022819372533154484\n",
      "gef_classifier has_extrinsic_and_fully_factual: 0.44 (baseline diff: 0.0)\n",
      "gef_classifier has_extrinsic_and_fully_factual baseline diff mcnemar p-value: 1.0\n",
      "gef_classifier has_extrinsic_and_fully_factual baseline diff wilcoxon p-value: 1.0\n",
      "\n",
      "gef_oracle is_factual: 0.67 (baseline diff: 0.25000000000000006)\n",
      "gef_oracle is_factual baseline diff mcnemar p-value: 5.960464477539063e-08\n",
      "gef_oracle is_factual baseline diff wilcoxon p-value: 5.733031437583867e-07\n",
      "gef_oracle has_extrinsic_and_fully_factual: 0.63 (baseline diff: 0.19)\n",
      "gef_oracle has_extrinsic_and_fully_factual baseline diff mcnemar p-value: 3.814697265625e-06\n",
      "gef_oracle has_extrinsic_and_fully_factual baseline diff wilcoxon p-value: 1.3071845366763019e-05\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/envs/factual-beam-search/lib/python3.8/site-packages/scipy/stats/_morestats.py:3159: UserWarning: Sample size too small for normal approximation.\n",
      "  warnings.warn(\"Sample size too small for normal approximation.\")\n",
      "/usr/local/anaconda3/envs/factual-beam-search/lib/python3.8/site-packages/scipy/stats/_morestats.py:3159: UserWarning: Sample size too small for normal approximation.\n",
      "  warnings.warn(\"Sample size too small for normal approximation.\")\n",
      "/usr/local/anaconda3/envs/factual-beam-search/lib/python3.8/site-packages/scipy/stats/_morestats.py:3159: UserWarning: Sample size too small for normal approximation.\n",
      "  warnings.warn(\"Sample size too small for normal approximation.\")\n",
      "/usr/local/anaconda3/envs/factual-beam-search/lib/python3.8/site-packages/scipy/stats/_morestats.py:3159: UserWarning: Sample size too small for normal approximation.\n",
      "  warnings.warn(\"Sample size too small for normal approximation.\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from statsmodels.stats.contingency_tables import mcnemar\n",
    "from scipy.stats import wilcoxon\n",
    "\n",
    "with open(\"results/evaluation/bart-test-extrinsic-100-summaries.json\", \"r\") as f:\n",
    "    json_sums = json.load(f)\n",
    "df = pd.DataFrame(json_sums.values(), index=list(json_sums.keys()))\n",
    "\n",
    "\n",
    "def get_baseline_diff(df, model, metric):\n",
    "    return pd.crosstab(index=df[f\"baseline-bart_{metric}\"], columns=df[f\"{model}_{metric}\"])\n",
    "\n",
    "for model in [\"corrector\", \"pinocchio\", \"rl-fact\", \"gef_classifier\", \"gef_oracle\"]:\n",
    "\n",
    "    for metric in [\"is_factual\", \"has_extrinsic_and_fully_factual\"]:\n",
    "        diff_metric = get_baseline_diff(df, model, metric)\n",
    "        baseline_mean = df[f\"baseline-bart_{metric}\"].mean()\n",
    "        model_mean = df[f\"{model}_{metric}\"].mean()\n",
    "        print(f\"{model} {metric}: {model_mean} (baseline diff: {model_mean - baseline_mean})\")\n",
    "        print(f\"{model} {metric} baseline diff mcnemar p-value: {mcnemar(diff_metric).pvalue}\")\n",
    "        wilcoxon_p = wilcoxon(\n",
    "            df[f\"baseline-bart_{metric}\"].astype(int), \n",
    "            df[f\"{model}_{metric}\"].astype(int)\n",
    "        ).pvalue\n",
    "        print(f\"{model} {metric} baseline diff wilcoxon p-value: {wilcoxon_p}\")\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BART 175"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rl-fact is_factual: 0.6057142857142858 (baseline diff: 0.16571428571428576)\n",
      "rl-fact is_factual baseline diff mcnemar p-value: 2.4299079086631536e-06\n",
      "rl-fact is_factual baseline diff wilcoxon p-value: 3.42197110897684e-06\n",
      "rl-fact has_extrinsic_and_fully_factual: 0.42857142857142855 (baseline diff: -0.051428571428571435)\n",
      "rl-fact has_extrinsic_and_fully_factual baseline diff mcnemar p-value: 0.19959086691960698\n",
      "rl-fact has_extrinsic_and_fully_factual baseline diff wilcoxon p-value: 0.14954135458461512\n",
      "\n",
      "gef_classifier is_factual: 0.52 (baseline diff: 0.08000000000000002)\n",
      "gef_classifier is_factual baseline diff mcnemar p-value: 0.0025768280029296875\n",
      "gef_classifier is_factual baseline diff wilcoxon p-value: 0.001745118699528905\n",
      "gef_classifier has_extrinsic_and_fully_factual: 0.4742857142857143 (baseline diff: -0.005714285714285672)\n",
      "gef_classifier has_extrinsic_and_fully_factual baseline diff mcnemar p-value: 1.0\n",
      "gef_classifier has_extrinsic_and_fully_factual baseline diff wilcoxon p-value: 0.8272593465627113\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from statsmodels.stats.contingency_tables import mcnemar\n",
    "from scipy.stats import wilcoxon\n",
    "\n",
    "with open(\"results/evaluation/bart-test-extrinsic-175-summaries.json\", \"r\") as f:\n",
    "    json_sums = json.load(f)\n",
    "df = pd.DataFrame(json_sums.values(), index=list(json_sums.keys()))\n",
    "\n",
    "\n",
    "def get_baseline_diff(df, model, metric):\n",
    "    return pd.crosstab(index=df[f\"baseline-bart_{metric}\"], columns=df[f\"{model}_{metric}\"])\n",
    "\n",
    "for model in [\"rl-fact\", \"gef_classifier\"]:\n",
    "\n",
    "    for metric in [\"is_factual\", \"has_extrinsic_and_fully_factual\"]:\n",
    "        diff_metric = get_baseline_diff(df, model, metric)\n",
    "        baseline_mean = df[f\"baseline-bart_{metric}\"].mean()\n",
    "        model_mean = df[f\"{model}_{metric}\"].mean()\n",
    "        print(f\"{model} {metric}: {model_mean} (baseline diff: {model_mean - baseline_mean})\")\n",
    "        print(f\"{model} {metric} baseline diff mcnemar p-value: {mcnemar(diff_metric).pvalue}\")\n",
    "        wilcoxon_p = wilcoxon(\n",
    "            df[f\"baseline-bart_{metric}\"].astype(int), \n",
    "            df[f\"{model}_{metric}\"].astype(int)\n",
    "        ).pvalue\n",
    "        print(f\"{model} {metric} baseline diff wilcoxon p-value: {wilcoxon_p}\")\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pegasus 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gef_classifier is_factual: 0.65 (baseline diff: 0.07000000000000006)\n",
      "gef_classifier is_factual baseline diff mcnemar p-value: 0.11846923828124999\n",
      "gef_classifier is_factual baseline diff wilcoxon p-value: 0.07070114486598297\n",
      "gef_classifier has_extrinsic_and_fully_factual: 0.54 (baseline diff: -0.06999999999999995)\n",
      "gef_classifier has_extrinsic_and_fully_factual baseline diff mcnemar p-value: 0.18924713134765625\n",
      "gef_classifier has_extrinsic_and_fully_factual baseline diff wilcoxon p-value: 0.12663045794761718\n",
      "\n",
      "gef_oracle is_factual: 0.79 (baseline diff: 0.21000000000000008)\n",
      "gef_oracle is_factual baseline diff mcnemar p-value: 9.5367431640625e-07\n",
      "gef_oracle is_factual baseline diff wilcoxon p-value: 4.592833711753968e-06\n",
      "gef_oracle has_extrinsic_and_fully_factual: 0.74 (baseline diff: 0.13)\n",
      "gef_oracle has_extrinsic_and_fully_factual baseline diff mcnemar p-value: 0.002349853515625\n",
      "gef_oracle has_extrinsic_and_fully_factual baseline diff wilcoxon p-value: 0.0016162222150599857\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from statsmodels.stats.contingency_tables import mcnemar\n",
    "from scipy.stats import wilcoxon\n",
    "\n",
    "with open(\"results/evaluation/pegasus-test-extrinsic-100-summaries.json\", \"r\") as f:\n",
    "    json_sums = json.load(f)\n",
    "df = pd.DataFrame(json_sums.values(), index=list(json_sums.keys()))\n",
    "\n",
    "\n",
    "def get_baseline_diff(df, model, metric):\n",
    "    return pd.crosstab(index=df[f\"baseline-pegasus_{metric}\"], columns=df[f\"{model}_{metric}\"])\n",
    "\n",
    "for model in [\"gef_classifier\", \"gef_oracle\"]:\n",
    "\n",
    "    for metric in [\"is_factual\", \"has_extrinsic_and_fully_factual\"]:\n",
    "        diff_metric = get_baseline_diff(df, model, metric)\n",
    "        baseline_mean = df[f\"baseline-pegasus_{metric}\"].mean()\n",
    "        model_mean = df[f\"{model}_{metric}\"].mean()\n",
    "        print(f\"{model} {metric}: {model_mean} (baseline diff: {model_mean - baseline_mean})\")\n",
    "        print(f\"{model} {metric} baseline diff mcnemar p-value: {mcnemar(diff_metric).pvalue}\")\n",
    "        wilcoxon_p = wilcoxon(\n",
    "            df[f\"baseline-pegasus_{metric}\"].astype(int), \n",
    "            df[f\"{model}_{metric}\"].astype(int)\n",
    "        ).pvalue\n",
    "        print(f\"{model} {metric} baseline diff wilcoxon p-value: {wilcoxon_p}\")\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BART 175, rl fact vs gef classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gef_classifier is_factual: 0.52 (rl-fact diff: -0.08571428571428574)\n",
      "gef_classifier is_factual rl-fact diff mcnemar p-value: 0.016673847800120715\n",
      "gef_classifier is_factual rl-fact diff wilcoxon p-value: 0.011229886652916691\n",
      "gef_classifier has_extrinsic_and_fully_factual: 0.4742857142857143 (rl-fact diff: 0.04571428571428576)\n",
      "gef_classifier has_extrinsic_and_fully_factual rl-fact diff mcnemar p-value: 0.26818725105476915\n",
      "gef_classifier has_extrinsic_and_fully_factual rl-fact diff wilcoxon p-value: 0.2059032107320684\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from statsmodels.stats.contingency_tables import mcnemar\n",
    "from scipy.stats import wilcoxon\n",
    "\n",
    "with open(\"results/evaluation/bart-test-extrinsic-175-summaries.json\", \"r\") as f:\n",
    "    json_sums = json.load(f)\n",
    "df = pd.DataFrame(json_sums.values(), index=list(json_sums.keys()))\n",
    "\n",
    "\n",
    "def get_rl_fact_diff(df, model, metric):\n",
    "    return pd.crosstab(index=df[f\"rl-fact_{metric}\"], columns=df[f\"{model}_{metric}\"])\n",
    "\n",
    "for model in [\"gef_classifier\"]:\n",
    "\n",
    "    for metric in [\"is_factual\", \"has_extrinsic_and_fully_factual\"]:\n",
    "        diff_metric = get_rl_fact_diff(df, model, metric)\n",
    "        rl_fact_mean = df[f\"rl-fact_{metric}\"].mean()\n",
    "        model_mean = df[f\"{model}_{metric}\"].mean()\n",
    "        print(f\"{model} {metric}: {model_mean} (rl-fact diff: {model_mean - rl_fact_mean})\")\n",
    "        print(f\"{model} {metric} rl-fact diff mcnemar p-value: {mcnemar(diff_metric).pvalue}\")\n",
    "        wilcoxon_p = wilcoxon(\n",
    "            df[f\"rl-fact_{metric}\"].astype(int), \n",
    "            df[f\"{model}_{metric}\"].astype(int)\n",
    "        ).pvalue\n",
    "        print(f\"{model} {metric} rl-fact diff wilcoxon p-value: {wilcoxon_p}\")\n",
    "\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3f3fe5eacab954956417c9014984aac9d559687fff881692b0d4e46fa0e895bd"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('factual-beam-search')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
